<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-7580334-2');
    </script>

	<script src="https://cdn.staticfile.org/jquery/2.1.1/jquery.min.js"></script>
	<script src="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/js/bootstrap.min.js"></script>

    <title>Shengyu Huang</title>

    <meta name="author" content="Shengyu Huang">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
    <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:78%;vertical-align:top">
                                    <p style="text-align:center">
                                        <name>Shengyu Huang 黄盛宇</name>
                                    </p>
                                    <p style="text-align:justify; text-justify:inter-ideograph;">
                                        I am a Research Scientist at <a href="https://research.nvidia.com/labs/dvl/#about">Nvidia Dynamic Vision and Learning Group</a>. I received my PhD from ETH Zürich in 2024, mainly advised by <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>. Previously, I was an intern at NVIDIA and Google. I obtained my master degree in Geomatics Engineering from ETH Zürich. My undergraduate study was done in Shanghai, where I spent my first year in Mathematics but changed my direction and completed a bachelor in Surveying and Mapping Engineering at Tongji University.
                                    <p style="text-align:center">
                                        <a href="mailto:shengyuh@nvidia.com">Email</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=bBn1d1oAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                                        <a href="https://github.com/ShengyuH">Github</a>&nbsp/&nbsp
                                        <a href="https://www.linkedin.com/in/shengyu-huang-2a50b2158/">LinkedIn</a> &nbsp/&nbsp
                                        <a href="https://twitter.com/ShengyHuang">Twitter</a> &nbsp
                                        <!-- <a href="assets/resume.pdf">CV</a> -->
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:30%;max-width:30%">
                                    <a href="thumbnail/portrait_autumn.jpg"><img style="width:150%;max-width:150%" alt="profile photo" src="thumbnail/portrait_autumn.jpg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table width="60%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tr>
                        <td width="12%" valign="middle">
                            <a href="https://www.tongji.edu.cn"><img src="thumbnail/tongji_logo.png" width="60"></a>
                            </td>
                          <td width="12%" valign="middle">
                            <a href="https://ethz.ch/en.html"><img src="thumbnail/eth_logo.png" width="120"></a>
                          </td>
                          <td width="12%" valign="middle">
                            <a href="https://nv-tlabs.github.io"><img src="thumbnail/nvidia_logo.png" width="75"></a>
                          </td>
                          <td width="12%" valign="middle">
                            <a href="https://research.google/"><img src="thumbnail/google_logo.png" width="50"></a>
                          </td>

                        </tr>
                        </table>
                  
                    
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Publications</heading>
                                    <p style="text-align:justify; text-justify:inter-ideograph;">
                                        My research interest lies in 3D vision, machine learning and robotics. If you find my work interesting and would like to chat or want to do an internship with me, feel free to shoot me an e-mail!</p>
                                    <p style="text-align:justify; text-justify:inter-ideograph;">
                                        * denotes equal contribution; <sup>#</sup> denotes mentored students.</p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>   
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/cosmos-dream-drive.gif" alt="clean-usnob" width="180" height="100">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models</papertitle>
                                    </a>
                                    <br>
                                    <strong>Core contributor</strong>. I led the post-training of Cosmos WFM for Lidar generation including data curation, tokenizer and diffusion model training.
                                    <br>
                                    <em>White paper</em>, 2025
                                    <br>
                                    <a href="https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams">project</a> /
                                    <a href="https://arxiv.org/abs/2506.09042">paper</a> /
                                    <a href="https://github.com/nv-tlabs/Cosmos-Drive-Dreams">code</a> 
                                    <p></p>
                                </td>
                            </tr>


                            
                            
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/rpf.gif" alt="clean-usnob" width="180" height="100">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>Rectified Point Flow: Generic Point Cloud Pose Estimation</papertitle>
                                    </a>
                                    <br>
                                    Tao Sun*,
                                    Liyuan Zhu*,
                                    <strong>Shengyu Huang</strong>,
                                    Shuran Song,
                                    Iro Armeni
                                    <br>
                                    <em>arXiv</em>, 2025
                                    <br>
                                    <a href="https://arxiv.org/abs/2506.05282">paper</a> /
                                    <a href="https://github.com/GradientSpaces/Rectified-Point-Flow">code</a> /
                                    <a href="https://rectified-pointflow.github.io">project</a> /
                                    <a href="bibtex/sun2025rpf.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>


                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/marigold_pami.gif" alt="clean-usnob" width="180" height="180">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis</papertitle>
                                    </a>
                                    <br>
                                    Bingxin Ke*, 
                                    Kevin Qu*, 
                                    Tianfu Wang*, 
                                    Nando Metzger*, 
                                    <strong>Shengyu Huang</strong>, 
                                    Bo Li, 
                                    Anton Obukhov, 
                                    Konrad Schindler
                                    <br>
                                    <em>PAMI</em>, 2025
                                    <br>
                                    <a href="https://arxiv.org/abs/2505.09358">paper</a> /
                                    <a href="https://github.com/prs-eth/marigold">code</a> /
                                    <a href="https://www.obukhov.ai/marigold">project</a> /
                                    <a href="bibtex/ke2025marigold_journal.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/restyle3d_cropped.gif" alt="clean-usnob" width="200" height="112"> 
                                </td>
               
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>ReStyle3D: Scene-level Appearance Transfer with Semantic Correspondences</papertitle>
                                    </a>
                                    <br>
                                    Liyuan Zhu, 
                                    Shengqu Cai*, 
                                    <strong>Shengyu Huang*</strong>, 
                                    Gordon Wetzstein, 
                                    Naji Khosravan, 
                                    Iro Armeni
                                    <br>
                                    <em>Siggraph</em>, 2025
                                    <br>
                                    <a href="https://arxiv.org/abs/2502.10377">paper</a> /
                                    <a href="https://github.com/ReStyle3D/ReStyle3D">code</a> /
                                    <a href="https://restyle3d.github.io">project</a> /
                                    <a href="bibtex/zhu2025restyle3d.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/rollingdepth.gif" alt="clean-usnob" width="200" height="112">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>RollingDepth: Video Depth without Video Models</papertitle>
                                    </a>
                                    <br>
                                    Bingxin Ke, 
                                    Dominik Narnhofer, 
                                    <strong>Shengyu Huang</strong>, 
                                    Lei Ke, 
                                    Torben Peters, 
                                    Katerina Fragkiadaki, 
                                    Anton Obukhov, 
                                    Konrad Schindler
                                    <br>
                                    <em>CVPR</em>, 2025
                                    <br>
                                    <a href="https://arxiv.org/abs/2411.19189">paper</a> /
                                    <a href="https://github.com/prs-eth/rollingdepth">code</a> /
                                    <a href="https://rollingdepth.github.io">project</a> /
                                    <a href="bibtex/ke2025rolling.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>


                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/loopsplat.gif" alt="clean-usnob" width="200" height="112">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>LoopSplat: Loop Closure by Registering 3D Gaussian Splats</papertitle>
                                    </a>
                                    <br>
                                    Liyuan Zhu, Yue Li, Erik sandström, <strong>Shengyu Huang</strong>, Konrad Schindler, Iro Armeni
                                    <br>
                                    <em>3DV</em>, 2025, <a style="color:#FF0000" ;> (oral)</a> 
                                    <br>
                                    <a href="https://arxiv.org/abs/2408.10154">paper</a> /
                                    <a href="https://github.com/GradientSpaces/LoopSplat">code</a> /
                                    <a href="https://loopsplat.github.io">project</a> /
                                    <a href="bibtex/zhu2024loopsplat.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/4_3/nss.png" alt="clean-usnob" width="200" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>NSS: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change</papertitle>
                                    </a>
                                    <br>
                                    Tao Sun<sup>#</sup>, Yan Hao<sup>#</sup>, <strong>Shengyu Huang</strong>, Silvio Savarese, Konrad Schindler, Marc Pollefeys, Iro Armeni
                                    <br>
                                    <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, 2025
                                    <br>
                                    <a href="https://arxiv.org/abs/2311.09346">paper</a> /
                                    <a>code(coming soon)</a> /
                                    <a href="http://nothing-stands-still.com">project</a> 
                                    <br>Supervised student project for 3DV at ETH Zurich.<br>
                                    <p></p>
                                </td>
                            </tr>                           

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/4_3/dginstyle.png" alt="clean-usnob" width="200" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control</papertitle>
                                    </a>
                                    <br>
                                    Yuru Jia<sup>#</sup>, Lukas Hoyer, <strong>Shengyu Huang</strong>, Tianfu Wang, Luc Van Gool, Konrad Schindler, Anton Obukhov
                                    <br>
                                    <em>ECCV</em>, 2024
                                    <br>
                                    <a href="https://arxiv.org/abs/2312.03048">paper</a> /
                                    <a href="https://dginstyle.github.io">project</a> /
                                    <a>code (coming soon)</a> /
                                    <a href="bibtex/huang2023nfl.txt">bibtex</a>
                                    <br>Supervised master thesis at ETH Zurich. <br>
                                    <p></p>
                                </td>
                            </tr> 


                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/4_3/marigold.png" alt="clean-usnob" width="200" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation</papertitle>
                                    </a>
                                    <br>
                                    Bingxin Ke, Anton Obukhov, <strong>Shengyu Huang</strong>, Nando Metzger, Rodrigo Caye Daudt, Konrad Schindler
                                    <br>
                                    <em>CVPR</em>, 2024, <a style="color:#FF0000" ;> (oral, Best Paper Award Candidate)</a>
                                    <br>
                                    <a href="https://arxiv.org/abs/2312.02145">paper</a> /
                                    <a href="https://github.com/prs-eth/marigold">code</a> /
                                    <a href="https://marigoldmonodepth.github.io/">project</a> /
                                    <a href="bibtex/ke2023marigold.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/4_3/livingscene.png" alt="clean-usnob" width="200" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>Living Scenes: Multi-object Relocalization and Reconstruction in Changing 3D Environments</papertitle>
                                    </a>
                                    <br>
                                    Liyuan Zhu<sup>#</sup>, <strong>Shengyu Huang</strong>, Konrad Schindler, Iro Armeni
                                    <br>
                                    <em>CVPR</em>, 2024, <a style="color:#FF0000" ;> (highlight)</a>
                                    <br>
                                    <a href="https://arxiv.org/abs/2312.09138">paper</a> /
                                    <a href="https://github.com/GradientSpaces/LivingScenes">code</a> /
                                    <a href="https://www.zhuliyuan.net/livingscenes">project</a> /
                                    <a href="bibtex/zhu2023livingscene.txt">bibtex</a>
                                    <br>Supervised master thesis at ETH Zurich. <br>
                                    <p></p>
                                </td>
                            </tr>


                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/2_1/dynfl.png" alt="clean-usnob" width="200" height="100">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>Dynamic LiDAR Re-simulation using Compositional Neural Fields</papertitle>
                                    </a>
                                    <br>
                                    Hanfeng Wu<sup>#</sup>, Xingxing Zuo, Stefan Leutenegger, Or Litany, Konrad Schindler, <strong>Shengyu Huang</strong>
                                    <br>
                                    <em>CVPR</em>, 2024, <a style="color:#FF0000" ;> (highlight)</a> 
                                    <br>
                                    <em>CVPR Data-Driven Autonomous Driving Simulation Workshop</em>, 2024, <a style="color:#FF0000" ;> (oral)</a>
                                    <br>
                                    <a href="https://arxiv.org/abs/2312.05247">paper</a> /
                                    <a href="https://github.com/prs-eth/DyNFL-Dynamic-LiDAR-Re-simulation-using-Compositional-Neural-Fields">code</a> /
                                    <a href="dynfl/index.html">project</a> /
                                    <a href="bibtex/wu2023dynfl.txt">bibtex</a>
                                    <br>Supervised master thesis at ETH Zurich.<br>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/2_1/nfl.png" alt="clean-usnob" width="200" height="100">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>Neural LiDAR Fields for Novel View Synthesis</papertitle>
                                    </a>
                                    <br>
                                    <strong>Shengyu Huang</strong>, Zan Gojcic, Zian Wang, Francis Williams, Yoni Kasten, Sanja Fidler, Konrad Schindler, Or Litany
                                    <br>
                                    <em>ICCV</em>, 2023
                                    <br>
                                    <em>ICCV Neural Fields for Autonomous Driving and Robotics Workshop, 2023,<a style="color:#FF0000" ;> (oral)</a>
                                    <br>
                                    <a href="https://arxiv.org/abs/2305.01643">paper</a> /
                                    <a href="https://research.nvidia.com/labs/toronto-ai/nfl/">project</a> /
                                    <a href="assets/nfl_poster.pdf">poster</a> /
                                    <a href="bibtex/huang2023nfl.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/2_1/fegr.png" alt="clean-usnob" width="200" height="100">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes</papertitle>
                                    </a>
                                    <br>
                                    Zian Wang, Tianchang Shen, Jun Gao, <strong>Shengyu Huang</strong>, Jacob Munkberg, Jon Hasselgren, Zan Gojcic, Wenzheng Chen, Sanja Fidler
                                    <br>
                                    <em>CVPR</em>, 2023
                                    <br>
                                    <a href="https://arxiv.org/abs/2304.03266">paper</a> /
                                    <a href="https://research.nvidia.com/labs/toronto-ai/fegr/">project</a> /
                                    <a href="https://www.youtube.com/watch?v=1KvHY3tlhhY&feature=youtu.be">video</a>/
                                    <a href="bibtex/wang2023fegr.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/4_3/deflow.png" alt="clean-usnob" width="200" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>DEFLOW: Self-supervised 3D Motion Estimation of Debris Flow</papertitle>
                                    </a>
                                    <br>
                                    Liyuan Zhu<sup>#</sup>, Yuru Jia<sup>#</sup>, <strong>Shengyu Huang</strong>, Nicholas Meyer, Andreas Wieser, Konrad Schindler, Jordan Aaron
                                    <br>
                                    <em>CVPR Workshop</em>, 2023, <a style="color:#FF0000" ;> (Best Paper Award)</a>
                                    <br>
                                    <a href="https://arxiv.org/abs/2304.02569">paper</a> /
                                    <a href="https://github.com/prs-eth/DeFlow">code</a> /
                                    <a href="https://www.zhuliyuan.net/deflow">project</a> /
                                    <a href="assets/deflow_poster.pdf"><i class="fa fa-file-pdf-o ai-3x"></i>poster</a>/
                                    <a href="bibtex/zhu2023deflow.txt">bibtex</a>
                                    <br>Supervised semester project at ETH Zurich.<br>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/2_1/pc_accumulation.png" alt="clean-usnob" width="200" height="100">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>Dynamic 3D Scene Analysis by Point Cloud Accumulation</papertitle>
                                    </a>
                                    <br>
                                    <strong>Shengyu Huang</strong>, Zan Gojcic, Jiahui Huang, Andreas Wieser, Konrad Schindler
                                    <br>
                                    <em>ECCV</em>, 2022
                                    <br>
                                    <a href="http://arxiv.org/abs/2207.12394">paper</a> /
                                    <a href="https://github.com/prs-eth/PCAccumulation">code</a> /
                                    <a href="pcaccumulation/index.html">project</a> /
                                    <a href="https://youtu.be/UFgXNOv1cZo">video</a>/
                                    <a href="pcaccumulation/assets/poster.pdf"><i class="fa fa-file-pdf-o ai-3x"></i>poster</a>/
                                    <a href="bibtex/huang2022accumulation.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/2_1/implicity.png" alt="clean-usnob" width="200" height="100">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>ImpliCity: City Modeling from Satellite with Deep Implicit Occupancy Fields</papertitle>
                                    </a>
                                    <br>
                                    Corinne Stucker, Bingxin Ke<sup>#</sup>, Yuanwen Yue<sup>#</sup>, <strong>Shengyu Huang</strong>, Iro Armeni, Konrad Schindler
                                    <br>
                                    <em>ISPRS Congress</em>, 2022, <a style="color:#FF0000" ;> (Best Young Author Award) </a>
                                    <br>
                                    <a href="https://arxiv.org/abs/2201.09968">paper</a> /
                                    <a href="https://github.com/prs-eth/ImpliCity">code</a> /
                                    <a href="bibtex/stucker2022implicity.txt">bibtex</a>
                                    <br>Supervised semester project at ETH Zurich.<br>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/2_1/predator.png" alt="clean-usnob" width="200" height="100">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>PREDATOR: Registration of 3D Point Clouds with Low Overlap</papertitle>
                                    </a>
                                    <br>
                                    <strong>Shengyu Huang*</strong>, Zan Gojcic*, Mikhail Usvyatsov, Andreas Wieser, Konrad Schindler
                                    <br>
                                    <em>CVPR</em>, 2021,<a style="color:#FF0000" ;> (oral)</a>
                                    <br>
                                    <a href="https://arxiv.org/pdf/2011.13005.pdf">paper</a> /
                                    <a href="https://github.com/ShengyuH/OverlapPredator">code</a> /
                                    <a href="predator/index.html">project</a> /
                                    <a href="https://www.youtube.com/watch?v=5L2vtuQL8Lg">video</a>/
                                    <a href="predator/assets/predator_poster.pdf"><i class="fa fa-file-pdf-o ai-3x"></i>poster</a>/
                                    <a href="https://www.youtube.com/watch?v=AZQGJa6R_4I&t=1559s">talk(中文)</a> /
                                    <a href="bibtex/huang2020predator.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>
                            
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="thumbnail/papers/4_3/iros.png" alt="clean-usnob" width="200" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <a>
                                        <papertitle>Indoor Scene Recognition in 3D</papertitle>
                                    </a>
                                    <br>
                                    <strong>Shengyu Huang</strong>, Mikhail Usvyatsov, Konrad Schindler
                                    <br>
                                    <em>IROS</em>, 2020
                                    <br>
                                    <a href="https://arxiv.org/pdf/2002.12819.pdf">paper</a> /
                                    <a href="https://github.com/ShengyuH/Scene-Recognition-in-3D">code</a> /
                                    <a href="https://www.youtube.com/watch?v=TtOJUbBkedw&t=13s">video</a> /
                                    <a href="bibtex/huang2020indoor.txt">bibtex</a>
                                    <p></p>
                                </td>
                            </tr>

                           

                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Experience</heading>
                                    <p>Research Scientist, NVIDIA, Dec. 2024 - now</p></p>
                                    <p>Ph.D. student, ETH Zurich, Oct. 2020 - Nov. 2024</p>
                                    <p>Student Researcher, Google, July - December. 2023</p>
                                    <p>Research Scientist intern, NVIDIA Toronto AI Lab, April - Dec. 2022</p>
                                    <p>M.Sc. in Geomatics, ETH Zurich, Sep. 2018 - Aug. 2020</p>
                                    <p>B.Eng. in Surveying and Mapping Enginnering, Tongji University, Sep. 2015 - June 2018</p>
                                    <p>Freshman, Mathematics, Tongji University, Sep. 2014 - Sep. 2015</p>

                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Academic Service</heading>
                                    
                                    <p><b>Workshop organizer: </b> <a href="https://neural-fields-beyond-cams.github.io">Neural Fields Beyond Conventional Camera at ECCV'24</a></p>
                                    <p><b>Conference reviewer: </b> Siggraph, CVPR, ICCV, ECCV, 3DV, IROS</p>
                                    <p><b>Journal reviewer: </b> ToG, T-PAMI, RA-L, ISPRS Journal</p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Misc</heading>
                                    <p style="text-align:justify; text-justify:inter-ideograph;">
                                        I was a die-hard Lakers fan, Kobe Bryant inspires me the most so far. I think <a href="https://open.spotify.com/artist/7tuWI2luTp61HHGmviWid8?si=-MRdq51sQj6VbYX9r7nSiQ">罗大佑</a> is the greatest Chinese singer. Besides research / work, I like running, hiking, photography, skiing, and playing basketball.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        <br> Last update: April. 2025. Thanks for the <a href="https://github.com/jonbarron/website">template</a>.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
        </tbody>
    </table>
    </td>
    </tr>
    </table>
</body>

</html>